{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7407df45514c4d78946ccef2ea6a3e25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_65a1f6d05a364cbe88c8263379717b9e","IPY_MODEL_1311295da35645d78b13c89684b005a6","IPY_MODEL_aea3bc5700a5448991aa4c0108bb8a34"],"layout":"IPY_MODEL_234454c785e5410e8a0ed1f04480166e"}},"65a1f6d05a364cbe88c8263379717b9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7bc8293513545ceb20ce970589ce054","placeholder":"​","style":"IPY_MODEL_dcf32289e4b948dfa79aa0bcb0750968","value":"config.json: 100%"}},"1311295da35645d78b13c89684b005a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1feaa187ed6141c2934b12f67371e935","max":69979,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0691a326956c44b188fad5d9a295e84c","value":69979}},"aea3bc5700a5448991aa4c0108bb8a34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fe0ee609f434cfcb4ffd3eb1700c689","placeholder":"​","style":"IPY_MODEL_b16da5b722c24491aa5f780175b0eb6b","value":" 70.0k/70.0k [00:00&lt;00:00, 2.69MB/s]"}},"234454c785e5410e8a0ed1f04480166e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7bc8293513545ceb20ce970589ce054":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcf32289e4b948dfa79aa0bcb0750968":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1feaa187ed6141c2934b12f67371e935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0691a326956c44b188fad5d9a295e84c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4fe0ee609f434cfcb4ffd3eb1700c689":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b16da5b722c24491aa5f780175b0eb6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fefdad23dc0145e68f983d776e9360ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d88c68573b14b099c31418a1c9f00e4","IPY_MODEL_dd05543d89a04721b2709207a5c14a77","IPY_MODEL_4028a3db2f134a11af64c856e594dfaf"],"layout":"IPY_MODEL_6344630d3fd2485e96aa180723183217"}},"7d88c68573b14b099c31418a1c9f00e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df1039ada05d45c58ef99a56eda0c89b","placeholder":"​","style":"IPY_MODEL_f79a0ccb71ee4f8d97fc227b2b87bcd2","value":"pytorch_model.bin: 100%"}},"dd05543d89a04721b2709207a5c14a77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e75aac31262d4f3f89af7a06a11380d8","max":22481227,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ce81594b5f34bf9aa8f1b4fce79e932","value":22481227}},"4028a3db2f134a11af64c856e594dfaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_844b4700e92d4f75811996134933adb5","placeholder":"​","style":"IPY_MODEL_b7cc737723ca4071aa99245d2f610226","value":" 22.5M/22.5M [00:00&lt;00:00, 75.0MB/s]"}},"6344630d3fd2485e96aa180723183217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df1039ada05d45c58ef99a56eda0c89b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f79a0ccb71ee4f8d97fc227b2b87bcd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e75aac31262d4f3f89af7a06a11380d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ce81594b5f34bf9aa8f1b4fce79e932":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"844b4700e92d4f75811996134933adb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7cc737723ca4071aa99245d2f610226":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3LXQE-OKDEV","outputId":"86ab1078-1d20-4e42-c80c-b3c9e5e7d0d2","executionInfo":{"status":"ok","timestamp":1733078350139,"user_tz":480,"elapsed":131410,"user":{"displayName":"Arnib Quazi","userId":"18254366215681113186"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/elmadafri/the-wildfire-dataset?dataset_version_number=1...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.94G/9.94G [01:19<00:00, 134MB/s] "]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/elmadafri/the-wildfire-dataset/versions/1\n"]}],"source":["import kagglehub\n","\n","# Download selected version\n","path = kagglehub.dataset_download(\"elmadafri/the-wildfire-dataset/versions/1\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","source":["!rm /root/.cache/kagglehub/datasets/elmadafri/the-wildfire-dataset/versions/1/the_wildfire_dataset/the_wildfire_dataset/val/fire/Both_smoke_and_fire/desktop.ini"],"metadata":{"id":"CzNEdDAsKNer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, models, transforms\n","from transformers import MobileViTForImageClassification, MobileViTImageProcessor"],"metadata":{"id":"6deUwDFwKNvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use pretrained weights\n","model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":185,"referenced_widgets":["7407df45514c4d78946ccef2ea6a3e25","65a1f6d05a364cbe88c8263379717b9e","1311295da35645d78b13c89684b005a6","aea3bc5700a5448991aa4c0108bb8a34","234454c785e5410e8a0ed1f04480166e","b7bc8293513545ceb20ce970589ce054","dcf32289e4b948dfa79aa0bcb0750968","1feaa187ed6141c2934b12f67371e935","0691a326956c44b188fad5d9a295e84c","4fe0ee609f434cfcb4ffd3eb1700c689","b16da5b722c24491aa5f780175b0eb6b","fefdad23dc0145e68f983d776e9360ba","7d88c68573b14b099c31418a1c9f00e4","dd05543d89a04721b2709207a5c14a77","4028a3db2f134a11af64c856e594dfaf","6344630d3fd2485e96aa180723183217","df1039ada05d45c58ef99a56eda0c89b","f79a0ccb71ee4f8d97fc227b2b87bcd2","e75aac31262d4f3f89af7a06a11380d8","7ce81594b5f34bf9aa8f1b4fce79e932","844b4700e92d4f75811996134933adb5","b7cc737723ca4071aa99245d2f610226"]},"id":"NyuRPnuwKN5M","outputId":"efdcd8be-893a-46e4-ce29-b0fe1d117aa7","executionInfo":{"status":"ok","timestamp":1733078363930,"user_tz":480,"elapsed":2044,"user":{"displayName":"Arnib Quazi","userId":"18254366215681113186"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/70.0k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7407df45514c4d78946ccef2ea6a3e25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/22.5M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fefdad23dc0145e68f983d776e9360ba"}},"metadata":{}}]},{"cell_type":"code","source":["# Fire vs No Fire\n","in_features = model.classifier.in_features\n","\n","# Strip out original classifier\n","model.classifier = nn.Identity()\n","\n","# Binary Head\n","binary_head = nn.Sequential(\n","    nn.Linear(in_features, 1),\n","    nn.Sigmoid()\n",")\n","\n","# Multi-class Head -> 5 output classes\n","multi_class_head = nn.Linear(in_features, 5)"],"metadata":{"id":"4ePg17ovKOBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","\n","class CustomFireDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.image_paths = []\n","        self.binary_labels = []\n","        self.multi_class_labels = []\n","\n","        # Define mappings for binary and multi-class labels\n","        binary_label_mapping = {'fire': 1, 'nofire': 0}\n","        multi_class_mapping = {\n","            'fire': {'Both_smoke_and_fire': 0, 'Smoke_from_fires': 1},\n","            'nofire': {'Fire_confounding_elements': 2, 'Forested_areas_without_confounding_elements': 3, 'Smoke_confounding_elements': 4}\n","        }\n","\n","        # Traverse the root directory and collect image paths and labels\n","        for binary_label_name in os.listdir(root_dir):\n","            binary_label_path = os.path.join(root_dir, binary_label_name)\n","            if os.path.isdir(binary_label_path):\n","                # Assign binary label\n","                binary_label = binary_label_mapping[binary_label_name]\n","\n","                # Traverse subclasses\n","                for subclass_name in os.listdir(binary_label_path):\n","                    subclass_path = os.path.join(binary_label_path, subclass_name)\n","                    if os.path.isdir(subclass_path):\n","                        # Assign multi-class label\n","                        multi_class_label = multi_class_mapping[binary_label_name][subclass_name]\n","\n","                        # Collect all images in the subclass directory\n","                        for img_name in os.listdir(subclass_path):\n","                            img_path = os.path.join(subclass_path, img_name)\n","                            if os.path.isfile(img_path):\n","                                self.image_paths.append(img_path)\n","                                self.binary_labels.append(binary_label)\n","                                self.multi_class_labels.append(multi_class_label)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        binary_label = self.binary_labels[idx]\n","        multi_class_label = self.multi_class_labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, (torch.tensor(binary_label, dtype=torch.float), torch.tensor(multi_class_label, dtype=torch.long))\n"],"metadata":{"id":"Kz941LVZLPfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","val_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Load your datasets (replace \"path_to_train_dataset\" and \"path_to_val_dataset\" with your paths)\n","#train_dataset = datasets.ImageFolder(\"/root/.cache/kagglehub/datasets/elmadafri/the-wildfire-dataset/versions/3/the_wildfire_dataset_2n_version/train\", transform=train_transforms)\n","#val_dataset = datasets.ImageFolder(\"/root/.cache/kagglehub/datasets/elmadafri/the-wildfire-dataset/versions/3/the_wildfire_dataset_2n_version/val\", transform=val_transforms)\n","train_dataset = CustomFireDataset(root_dir=\"/root/.cache/kagglehub/datasets/elmadafri/the-wildfire-dataset/versions/1/the_wildfire_dataset/the_wildfire_dataset/train\", transform=train_transforms)\n","val_dataset = CustomFireDataset(root_dir=\"/root/.cache/kagglehub/datasets/elmadafri/the-wildfire-dataset/versions/1/the_wildfire_dataset/the_wildfire_dataset/val\", transform=val_transforms)\n","\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=10)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=10)"],"metadata":{"id":"-8dz4859KOIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion_binary = nn.BCELoss()\n","criterion_multi_class = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(\n","    list(model.parameters()) + list(binary_head.parameters()) + list(multi_class_head.parameters()),\n","    lr=0.01\n",")"],"metadata":{"id":"iXuMlojtLRPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","binary_head = binary_head.to(device)\n","multi_class_head = multi_class_head.to(device)"],"metadata":{"id":"-PzmTCN4LRVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combined_loss(binary_output, binary_target, multi_class_output, multi_class_target, alpha=0.5, beta=0.5):\n","    loss_binary = criterion_binary(binary_output, binary_target)\n","    loss_multi_class = criterion_multi_class(multi_class_output, multi_class_target)\n","    return alpha * loss_binary + beta * loss_multi_class"],"metadata":{"id":"2gicUGpYLRYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch_list = []\n","loss_list = []\n","train_binary_accuracy_list = []\n","train_multi_class_accuracy_list = []\n","val_binary_accuracy_list = []\n","val_multi_class_accuracy_list = []\n","\n","# Training loop\n","epochs = 30  # You can adjust the number of epochs based on your needs\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct_train_binary = 0\n","    total_train_binary = 0\n","    correct_train_multi_class = 0\n","    total_train_multi_class = 0\n","\n","    # Training Loop\n","    for images, (binary_labels, multi_class_labels) in train_loader:\n","        images = images.to(device)\n","        binary_labels = binary_labels.to(device).float()\n","        multi_class_labels = multi_class_labels.to(device).long()\n","\n","        optimizer.zero_grad()\n","        features = model(images)  # Extract features using MobileViT\n","        features = features.logits\n","        binary_output = binary_head(features)\n","        multi_class_output = multi_class_head(features)\n","\n","        # Compute the combined loss\n","        loss = combined_loss(binary_output.squeeze(), binary_labels, multi_class_output, multi_class_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        # Calculate training accuracy for binary classification\n","        predicted_binary = (binary_output.squeeze() > 0.5).int()  # Convert probabilities to binary predictions\n","        correct_train_binary += (predicted_binary == binary_labels.int()).sum().item()\n","        total_train_binary += binary_labels.size(0)\n","\n","        # Calculate training accuracy for multi-class classification\n","        _, predicted_multi_class = torch.max(multi_class_output, 1)\n","        correct_train_multi_class += (predicted_multi_class == multi_class_labels).sum().item()\n","        total_train_multi_class += multi_class_labels.size(0)\n","\n","    # Calculate the average loss and training accuracies\n","    avg_loss = running_loss / len(train_loader)\n","    train_binary_accuracy = 100 * correct_train_binary / total_train_binary\n","    train_multi_class_accuracy = 100 * correct_train_multi_class / total_train_multi_class\n","\n","    # Validation Loop\n","    model.eval()\n","    correct_val_binary = 0\n","    total_val_binary = 0\n","    correct_val_multi_class = 0\n","    total_val_multi_class = 0\n","    with torch.no_grad():\n","        for images, (binary_labels, multi_class_labels) in val_loader:\n","            images = images.to(device)\n","            binary_labels = binary_labels.to(device).float()\n","            multi_class_labels = multi_class_labels.to(device).long()\n","\n","            features = model(images)  # Extract features using MobileNetV3\n","            features = features.logits\n","            binary_output = binary_head(features)\n","            multi_class_output = multi_class_head(features)\n","\n","            # Calculate validation accuracy for binary classification\n","            predicted_binary = (binary_output.squeeze() > 0.5).int()\n","            correct_val_binary += (predicted_binary == binary_labels.int()).sum().item()\n","            total_val_binary += binary_labels.size(0)\n","\n","            # Calculate validation accuracy for multi-class classification\n","            _, predicted_multi_class = torch.max(multi_class_output, 1)\n","            correct_val_multi_class += (predicted_multi_class == multi_class_labels).sum().item()\n","            total_val_multi_class += multi_class_labels.size(0)\n","\n","    # Calculate validation accuracies\n","    val_binary_accuracy = 100 * correct_val_binary / total_val_binary\n","    val_multi_class_accuracy = 100 * correct_val_multi_class / total_val_multi_class\n","\n","    # Store the metrics\n","    epoch_list.append(epoch + 1)\n","    loss_list.append(avg_loss)\n","    train_binary_accuracy_list.append(train_binary_accuracy)\n","    train_multi_class_accuracy_list.append(train_multi_class_accuracy)\n","    val_binary_accuracy_list.append(val_binary_accuracy)\n","    val_multi_class_accuracy_list.append(val_multi_class_accuracy)\n","\n","    # Log the metrics\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    print(f\"  Loss: {avg_loss:.4f}\")\n","    print(f\"  Training Binary Accuracy: {train_binary_accuracy:.2f}%\")\n","    print(f\"  Training Multi-Class Accuracy: {train_multi_class_accuracy:.2f}%\")\n","    print(f\"  Validation Binary Accuracy: {val_binary_accuracy:.2f}%\")\n","    print(f\"  Validation Multi-Class Accuracy: {val_multi_class_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NKKMlV4LRb-","outputId":"a04746f2-3874-4470-834c-69d2205433d7","executionInfo":{"status":"ok","timestamp":1733081786603,"user_tz":480,"elapsed":480182,"user":{"displayName":"Arnib Quazi","userId":"18254366215681113186"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","  Loss: 1.0513\n","  Training Binary Accuracy: 64.39%\n","  Training Multi-Class Accuracy: 39.85%\n","  Validation Binary Accuracy: 59.70%\n","  Validation Multi-Class Accuracy: 37.56%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 2/30\n","  Loss: 0.9995\n","  Training Binary Accuracy: 65.08%\n","  Training Multi-Class Accuracy: 42.02%\n","  Validation Binary Accuracy: 67.66%\n","  Validation Multi-Class Accuracy: 44.03%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 3/30\n","  Loss: 0.9643\n","  Training Binary Accuracy: 67.94%\n","  Training Multi-Class Accuracy: 46.00%\n","  Validation Binary Accuracy: 46.77%\n","  Validation Multi-Class Accuracy: 31.09%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 4/30\n","  Loss: 0.9032\n","  Training Binary Accuracy: 72.23%\n","  Training Multi-Class Accuracy: 49.07%\n","  Validation Binary Accuracy: 74.13%\n","  Validation Multi-Class Accuracy: 51.00%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 5/30\n","  Loss: 0.8530\n","  Training Binary Accuracy: 74.14%\n","  Training Multi-Class Accuracy: 50.56%\n","  Validation Binary Accuracy: 73.63%\n","  Validation Multi-Class Accuracy: 50.00%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 6/30\n","  Loss: 0.8104\n","  Training Binary Accuracy: 77.11%\n","  Training Multi-Class Accuracy: 52.31%\n","  Validation Binary Accuracy: 75.37%\n","  Validation Multi-Class Accuracy: 52.49%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 7/30\n","  Loss: 0.7717\n","  Training Binary Accuracy: 80.34%\n","  Training Multi-Class Accuracy: 56.39%\n","  Validation Binary Accuracy: 76.62%\n","  Validation Multi-Class Accuracy: 51.99%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 8/30\n","  Loss: 0.7491\n","  Training Binary Accuracy: 80.50%\n","  Training Multi-Class Accuracy: 56.28%\n","  Validation Binary Accuracy: 78.61%\n","  Validation Multi-Class Accuracy: 45.77%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 9/30\n","  Loss: 0.7267\n","  Training Binary Accuracy: 81.19%\n","  Training Multi-Class Accuracy: 56.76%\n","  Validation Binary Accuracy: 77.86%\n","  Validation Multi-Class Accuracy: 57.96%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 10/30\n","  Loss: 0.7205\n","  Training Binary Accuracy: 81.19%\n","  Training Multi-Class Accuracy: 57.34%\n","  Validation Binary Accuracy: 75.37%\n","  Validation Multi-Class Accuracy: 54.23%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 11/30\n","  Loss: 0.6980\n","  Training Binary Accuracy: 82.09%\n","  Training Multi-Class Accuracy: 59.30%\n","  Validation Binary Accuracy: 65.42%\n","  Validation Multi-Class Accuracy: 41.54%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 12/30\n","  Loss: 0.7249\n","  Training Binary Accuracy: 81.66%\n","  Training Multi-Class Accuracy: 57.98%\n","  Validation Binary Accuracy: 75.62%\n","  Validation Multi-Class Accuracy: 51.74%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 13/30\n","  Loss: 0.6639\n","  Training Binary Accuracy: 83.62%\n","  Training Multi-Class Accuracy: 59.94%\n","  Validation Binary Accuracy: 73.13%\n","  Validation Multi-Class Accuracy: 57.21%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 14/30\n","  Loss: 0.6703\n","  Training Binary Accuracy: 83.15%\n","  Training Multi-Class Accuracy: 59.30%\n","  Validation Binary Accuracy: 58.21%\n","  Validation Multi-Class Accuracy: 39.55%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 15/30\n","  Loss: 0.6488\n","  Training Binary Accuracy: 84.47%\n","  Training Multi-Class Accuracy: 61.00%\n","  Validation Binary Accuracy: 81.09%\n","  Validation Multi-Class Accuracy: 60.95%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 16/30\n","  Loss: 0.6371\n","  Training Binary Accuracy: 84.68%\n","  Training Multi-Class Accuracy: 63.12%\n","  Validation Binary Accuracy: 82.59%\n","  Validation Multi-Class Accuracy: 61.69%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 17/30\n","  Loss: 0.6408\n","  Training Binary Accuracy: 85.43%\n","  Training Multi-Class Accuracy: 62.06%\n","  Validation Binary Accuracy: 79.60%\n","  Validation Multi-Class Accuracy: 59.45%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 18/30\n","  Loss: 0.6408\n","  Training Binary Accuracy: 85.80%\n","  Training Multi-Class Accuracy: 63.33%\n","  Validation Binary Accuracy: 81.84%\n","  Validation Multi-Class Accuracy: 61.94%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 19/30\n","  Loss: 0.6182\n","  Training Binary Accuracy: 85.96%\n","  Training Multi-Class Accuracy: 64.86%\n","  Validation Binary Accuracy: 82.34%\n","  Validation Multi-Class Accuracy: 62.44%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 20/30\n","  Loss: 0.6159\n","  Training Binary Accuracy: 86.06%\n","  Training Multi-Class Accuracy: 63.65%\n","  Validation Binary Accuracy: 78.61%\n","  Validation Multi-Class Accuracy: 54.48%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 21/30\n","  Loss: 0.5907\n","  Training Binary Accuracy: 86.70%\n","  Training Multi-Class Accuracy: 64.60%\n","  Validation Binary Accuracy: 82.59%\n","  Validation Multi-Class Accuracy: 66.92%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 22/30\n","  Loss: 0.5872\n","  Training Binary Accuracy: 85.74%\n","  Training Multi-Class Accuracy: 64.44%\n","  Validation Binary Accuracy: 81.59%\n","  Validation Multi-Class Accuracy: 57.46%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 23/30\n","  Loss: 0.5732\n","  Training Binary Accuracy: 86.75%\n","  Training Multi-Class Accuracy: 66.30%\n","  Validation Binary Accuracy: 75.37%\n","  Validation Multi-Class Accuracy: 56.22%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 24/30\n","  Loss: 0.5773\n","  Training Binary Accuracy: 86.70%\n","  Training Multi-Class Accuracy: 66.98%\n","  Validation Binary Accuracy: 78.36%\n","  Validation Multi-Class Accuracy: 58.46%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 25/30\n","  Loss: 0.5582\n","  Training Binary Accuracy: 87.44%\n","  Training Multi-Class Accuracy: 68.57%\n","  Validation Binary Accuracy: 83.33%\n","  Validation Multi-Class Accuracy: 66.67%\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26/30\n","  Loss: 0.5491\n","  Training Binary Accuracy: 87.18%\n","  Training Multi-Class Accuracy: 67.67%\n","  Validation Binary Accuracy: 83.83%\n","  Validation Multi-Class Accuracy: 67.41%\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27/30\n","  Loss: 0.5487\n","  Training Binary Accuracy: 87.65%\n","  Training Multi-Class Accuracy: 67.04%\n","  Validation Binary Accuracy: 81.59%\n","  Validation Multi-Class Accuracy: 62.94%\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28/30\n","  Loss: 0.5384\n","  Training Binary Accuracy: 88.82%\n","  Training Multi-Class Accuracy: 68.68%\n","  Validation Binary Accuracy: 82.09%\n","  Validation Multi-Class Accuracy: 63.93%\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29/30\n","  Loss: 0.5107\n","  Training Binary Accuracy: 88.77%\n","  Training Multi-Class Accuracy: 69.63%\n","  Validation Binary Accuracy: 79.60%\n","  Validation Multi-Class Accuracy: 60.70%\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30/30\n","  Loss: 0.5291\n","  Training Binary Accuracy: 88.71%\n","  Training Multi-Class Accuracy: 70.32%\n","  Validation Binary Accuracy: 82.59%\n","  Validation Multi-Class Accuracy: 64.18%\n"]}]},{"cell_type":"code","source":["# Save the model's state dictionary\n","torch.save(model.state_dict(), \"mobileVit_fire_multi_classifier.pth\")\n","print(\"Model saved successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AjvO1fiiZTnC","outputId":"d288e927-e3c9-4853-a803-263be7a53cbc","executionInfo":{"status":"ok","timestamp":1733081786603,"user_tz":480,"elapsed":4,"user":{"displayName":"Arnib Quazi","userId":"18254366215681113186"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved successfully!\n"]}]},{"cell_type":"code","source":["# Save the metrics to a CSV file\n","data = {\n","    'Epoch': epoch_list,\n","    'Loss': loss_list,\n","    'Train Binary Accuracy': train_binary_accuracy_list,\n","    'Train Multi-Class Accuracy': train_multi_class_accuracy_list,\n","    'Validation Binary Accuracy': val_binary_accuracy_list,\n","    'Validation Multi-Class Accuracy': val_multi_class_accuracy_list\n","}\n","\n","# Create a DataFrame from the dictionary\n","df = pd.DataFrame(data)\n","\n","# Save the DataFrame to a CSV file\n","df.to_csv('multi_training_results.csv', index=False)\n","print(\"Training results saved to 'training_results.csv' successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7UwKq5NZUTC","outputId":"373d3bc4-1bf2-49cb-8adb-46d8b4c8c8e0","executionInfo":{"status":"ok","timestamp":1733081786604,"user_tz":480,"elapsed":3,"user":{"displayName":"Arnib Quazi","userId":"18254366215681113186"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training results saved to 'training_results.csv' successfully!\n"]}]},{"cell_type":"code","source":["# Define transformations for the test set\n","test_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Load your test dataset\n","test_dataset = CustomFireDataset(root_dir=\"/root/.cache/kagglehub/datasets/elmadafri/the-wildfire-dataset/versions/1/the_wildfire_dataset/the_wildfire_dataset/test\", transform=test_transforms)\n","\n","# Create a DataLoader for the test dataset\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=10)"],"metadata":{"id":"A1yCdl3NZk4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n","import numpy as np\n","import torch\n","\n","# Lists to store true labels and predicted probabilities/scores for binary classification\n","true_binary_labels = []\n","predicted_binary_probs = []  # Probabilities for the positive class (fire)\n","\n","# Lists to store true labels and predicted labels for multi-class classification\n","true_multi_class_labels = []\n","predicted_multi_class_labels = []\n","\n","# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    for images, (binary_labels, multi_class_labels) in test_loader:\n","        images = images.to(device)\n","        binary_labels = binary_labels.to(device).float()\n","        multi_class_labels = multi_class_labels.to(device).long()\n","\n","        # Forward pass\n","        features = model(images)\n","        features = features.logits\n","        binary_output = binary_head(features)\n","        multi_class_output = multi_class_head(features)\n","\n","        # Get probabilities for the positive class (fire) in binary classification\n","        binary_probs = binary_output.squeeze().cpu().numpy()  # Probabilities from Sigmoid\n","\n","        # Get predicted class labels for multi-class classification\n","        _, predicted_multi_class = torch.max(multi_class_output, 1)\n","        predicted_multi_class = predicted_multi_class.cpu().numpy()\n","\n","        # Store true labels and predictions for binary classification\n","        true_binary_labels.extend(binary_labels.cpu().numpy())\n","        predicted_binary_probs.extend(binary_probs)\n","\n","        # Store true labels and predictions for multi-class classification\n","        true_multi_class_labels.extend(multi_class_labels.cpu().numpy())\n","        predicted_multi_class_labels.extend(predicted_multi_class)\n","\n","# Combine all data into a single DataFrame\n","data = pd.DataFrame({\n","    \"True Binary Labels\": true_binary_labels,\n","    \"Predicted Binary Probabilities\": predicted_binary_probs,\n","    \"True Multi-Class Labels\": true_multi_class_labels,\n","    \"Predicted Multi-Class Labels\": predicted_multi_class_labels\n","})\n","\n","data.to_csv(\"mtl_mobileVit_test_results.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_uWRd6gZlPG","outputId":"c309472e-a608-4670-f809-a382def5985b","executionInfo":{"status":"ok","timestamp":1733081815197,"user_tz":480,"elapsed":28595,"user":{"displayName":"Arnib Quazi","userId":"18254366215681113186"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# **METRICS**"],"metadata":{"id":"N16hmNWwiSYP"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, classification_report\n","\n","# Load the data\n","data = pd.read_csv(\"mtl_mobileVit_test_results.csv\")\n","\n","# Binary Classification Metrics\n","true_binary = np.array(data['True Binary Labels'])\n","pred_binary_probs = np.array(data['Predicted Binary Probabilities'])\n","pred_binary = (pred_binary_probs >= 0.5).astype(int)\n","\n","f1 = f1_score(true_binary, pred_binary)\n","precision = precision_score(true_binary, pred_binary)\n","recall = recall_score(true_binary, pred_binary)\n","accuracy = accuracy_score(true_binary, pred_binary)\n","roc_auc = roc_auc_score(true_binary, pred_binary_probs)\n","\n","print(\"Binary Classification Metrics:\")\n","print(f\"F1 Score: {f1:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"ROC-AUC: {roc_auc:.4f}\")\n","\n","# Multi-class Classification Metrics\n","true_multi = np.array(data['True Multi-Class Labels'])\n","pred_multi = np.array(data['Predicted Multi-Class Labels'])\n","\n","print(\"\\nMulti-class Classification Report:\")\n","print(classification_report(true_multi, pred_multi))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obsNY5QUf4t9","outputId":"7fcf3104-51f4-40f3-a3d8-d0b0a5256fad","executionInfo":{"status":"ok","timestamp":1733081815199,"user_tz":480,"elapsed":2,"user":{"displayName":"Arnib Quazi","userId":"18254366215681113186"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Binary Classification Metrics:\n","F1 Score: 0.8100\n","Precision: 0.8025\n","Recall: 0.8176\n","Accuracy: 0.8512\n","ROC-AUC: 0.9257\n","\n","Multi-class Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.51      0.58        59\n","           1       0.61      0.76      0.68       100\n","           2       0.55      0.62      0.58        52\n","           3       0.82      0.78      0.80       128\n","           4       0.64      0.55      0.59        71\n","\n","    accuracy                           0.68       410\n","   macro avg       0.66      0.64      0.65       410\n","weighted avg       0.68      0.68      0.67       410\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FDIN7SP2gzCa"},"execution_count":null,"outputs":[]}]}